{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3K5_4OIqHmBx"
      },
      "source": [
        "## **1. Setup**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.1. Install Packages**"
      ],
      "metadata": {
        "id": "su7CZP2tjjkG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install black[jupyter]\n",
        "!pip install imbalanced-learn\n",
        "!pip install nltk\n",
        "!pip install PySastrawi\n",
        "!pip install fasttext\n",
        "!pip install -U gensim\n",
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.id.300.bin.gz\n",
        "!gunzip cc.id.300.bin.gz\n",
        "!pip install -U symspellpy\n",
        "!wget https://dumps.wikimedia.org/idwiki/latest/idwiki-latest-pages-articles.xml.bx2\n",
        "!pip install Sastrawi\n",
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.id.300.vec.gz\n",
        "!gunzip cc.id.300.vec.gz\n",
        "!pip install wordcloud\n",
        "!pip install bpemb\n",
        "!pip install keras"
      ],
      "metadata": {
        "id": "hFkRgL1jjBUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRYDuasuH4q8"
      },
      "source": [
        "### **1.2. Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNxmSlCQH_sa"
      },
      "outputs": [],
      "source": [
        "# Scraping Data\n",
        "import requests as req\n",
        "from bs4 import BeautifulSoup as bs\n",
        "from datetime import datetime\n",
        "import csv\n",
        "from google.colab import data_table, drive\n",
        "import pandas as pd\n",
        "hades = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36'}\n",
        "\n",
        "# Import Dataset\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from google.colab import data_table, drive\n",
        "\n",
        "# Text Preprocessing\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Word Embedding\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import gzip\n",
        "from urllib.request import urlopen\n",
        "\n",
        "# Oversampling\n",
        "from imblearn.over_sampling import BorderlineSMOTE\n",
        "\n",
        "# Pemodelan Bi-LSTM\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from keras.models import load_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBjFLKyMIE_m"
      },
      "source": [
        "### **1.3. Colab Configuration**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkPqMBV4IIUt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7b39f3d-1b9e-481f-f181-ded1487e2aef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# mount Drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3S0wICHIX2b"
      },
      "source": [
        "### **1.4. Using GPU for Training**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check Tensorflow version\n",
        "print(\"TensorFlow version: \" + tf.__version__)\n",
        "\n",
        "# check PyTorch version\n",
        "print(\"PyTorch version: \" + torch.__version__)\n",
        "\n",
        "# get the GPU device name\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# the device name should look like the following\n",
        "if device_name != \"/device:GPU:0\":\n",
        "    print(\n",
        "        \"\\n\\nThis error most likely means that this notebook is not \"\n",
        "        \"configured to use a GPU.  Change this in Notebook Settings via the \"\n",
        "        \"command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n\"\n",
        "    )\n",
        "    raise SystemError(\"GPU device not found\")\n",
        "print(\"Found GPU at: {}\".format(device_name))"
      ],
      "metadata": {
        "id": "cXQbem70i5qY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Scraping Data**"
      ],
      "metadata": {
        "id": "lWHEU7S-jjOh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.1. CNN**"
      ],
      "metadata": {
        "id": "1kQNrPPJjnOl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_cnn(start_page, end_page):\n",
        "    global hades\n",
        "    a = 1\n",
        "    for page in range(start_page, end_page + 1):\n",
        "        url = f'https://www.cnnindonesia.com/pemilu/indeks/616/{page}'\n",
        "        ge = req.get(url, hades).text\n",
        "        sop = bs(ge, 'lxml')\n",
        "        li = sop.find('div', class_='flex flex-col gap-5')\n",
        "        lin = li.find_all('article', class_='flex-grow')\n",
        "        for x in lin:\n",
        "            link = x.find('a')['href']\n",
        "            headline = x.find('a').find('h2').text\n",
        "            ge_ = req.get(link, hades).text\n",
        "            sop_ = bs(ge_, 'lxml')\n",
        "            date = sop_.find('div', class_='text-cnn_grey text-sm mb-4')\n",
        "            content = sop_.find_all('div', class_='detail-text text-cnn_black text-sm grow min-w-0')\n",
        "            for x in content:\n",
        "                x = x.find_all('p')\n",
        "                y = [y.text for y in x]\n",
        "                content_ = ''.join(y).replace('\\n', '').replace('ADVERTISEMENT', '').replace(\n",
        "                    'SCROLL TO CONTINUE WITH CONTENT', '')\n",
        "                print(f'done[{a}] > {headline[0:10]}')\n",
        "                a += 1\n",
        "                with open('cnn_pemilu.csv', 'a') as file:\n",
        "                    wr = csv.writer(file, delimiter=',')\n",
        "                    wr.writerow([headline, date, link, content_])"
      ],
      "metadata": {
        "id": "TxHTCG6uj5YZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scrape_cnn(131, 340) # Sesuaikan dengan halaman awal dan akhir yang ingin di-scrape"
      ],
      "metadata": {
        "id": "GKb9z-jTj6dJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.2. Kompas**"
      ],
      "metadata": {
        "id": "r0XqTWPAkH2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_kompas(start_page, end_page):\n",
        "    global hades\n",
        "    a = 1  # definisikan variabel a di sini\n",
        "    data = []\n",
        "\n",
        "    for page in range(start_page, end_page + 1):\n",
        "        url = f'https://pemilu.kompas.com/news?page={page}'\n",
        "\n",
        "        try:\n",
        "            ge = req.get(url, hades).text\n",
        "            sop = bs(ge, 'lxml')\n",
        "            li = sop.find('div', class_='list')\n",
        "            lin = li.find_all('a', class_='listLink display-flex')\n",
        "\n",
        "            for x in lin:\n",
        "                link = x.get('href', '')  # Get the href attribute or default to an empty string\n",
        "                if not link or link.startswith('#'):\n",
        "                    # Skip empty or invalid URLs (e.g., URLs starting with '#')\n",
        "                    continue\n",
        "\n",
        "                ge_ = req.get(link, headers=hades).text\n",
        "                sop_ = bs(ge_, 'lxml')\n",
        "                headline = sop_.find('h1', class_='read__title').text.strip()\n",
        "                date = sop_.find('div', class_='read__time').text.replace('WIB', '').replace('Kompas.com - ', '').strip()\n",
        "\n",
        "                content = sop_.find_all('div', class_='clearfix')\n",
        "                filtered_content = []\n",
        "                for x in content:\n",
        "                    paragraphs = x.find_all('p')\n",
        "                    for p in paragraphs:\n",
        "                        if 'ADVERTISEMENT' not in p.text and 'SCROLL TO RESUME CONTENT' not in p.text:\n",
        "                            clean_text = ' '.join(str(item) for item in p.contents if not isinstance(item, Tag))\n",
        "                            filtered_content.append(clean_text)\n",
        "\n",
        "                content_text = ''.join(filtered_content).replace('\\n', '')\n",
        "\n",
        "                data.append({'Title': headline, 'Date': date, 'Link': link, 'Content': content_text})\n",
        "                print(f'done[{a}] > {headline[0:20]}')\n",
        "                a += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            # Tangani kesalahan dengan mencetak pesan dan melanjutkan ke iterasi berikutnya\n",
        "            print(f\"Error: {e}\")\n",
        "            continue\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    csv_filename = dataset_path + 'kompas_pemilu2.csv'\n",
        "    df.to_csv(csv_filename, index=False)\n",
        "    print(f\"Data berhasil disimpan ke dalam {csv_filename}.\")"
      ],
      "metadata": {
        "id": "4zSHjccCkRMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scrape_kompas(701, 999)  # Sesuaikan dengan halaman awal dan akhir yang ingin di-scrape"
      ],
      "metadata": {
        "id": "1vw5iacDkWui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.3. Turnbackhoax**"
      ],
      "metadata": {
        "id": "R1UQFbNkk3aC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def scrape_turnbackhoax(keyword, start_page=1, end_page=float('inf')):\n",
        "    titles = []\n",
        "    contents = []\n",
        "    dates = []\n",
        "\n",
        "    page_num = start_page\n",
        "    while page_num <= end_page:\n",
        "        link = requests.get(f\"https://turnbackhoax.id/page/{page_num}/?s={keyword}\")\n",
        "        soup = BeautifulSoup(link.text, \"html.parser\")\n",
        "\n",
        "        # Extracting subpage links\n",
        "        subpage_links = [a['href'] for a in soup.select('.mh-loop-content h3 > a')]\n",
        "\n",
        "        if not subpage_links:\n",
        "            break  # Keluar dari loop jika tidak ada subpage lagi\n",
        "\n",
        "        for subpage_link in subpage_links:\n",
        "            # Navigate to the subpage\n",
        "            subpage = requests.get(subpage_link)\n",
        "            subpage_soup = BeautifulSoup(subpage.text, \"html.parser\")\n",
        "\n",
        "            # Extract title, date, and content from the subpage\n",
        "            title = subpage_soup.select_one('.entry-title')\n",
        "            date = subpage_soup.select_one('.entry-meta-date.updated')\n",
        "            content = subpage_soup.select_one('.entry-content')\n",
        "\n",
        "            if title and content and date:\n",
        "                titles.append(title.text.strip())\n",
        "                contents.append(content.text.strip())\n",
        "                dates.append(date.text.strip())\n",
        "\n",
        "        page_num += 1\n",
        "\n",
        "    df = pd.DataFrame({\"Title\": titles, \"Date\": dates, \"Content\": contents})\n",
        "    csv_filename = f'turnbackhoax_kampanye2.csv' #UBAH NAMA FILE\n",
        "    df.to_csv(csv_filename, index=False)\n",
        "    print(f\"Data berhasil disimpan ke dalam {csv_filename}.\")"
      ],
      "metadata": {
        "id": "1cPbzAbhkzf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scrape_turnbackhoax(\"pemilu\", 5, 8) # Sesuaikan dengan keyword, halaman awal dan akhir yang ingin di-scrape"
      ],
      "metadata": {
        "id": "LzTJN15Uk8eb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEBszomIJPGp"
      },
      "source": [
        "## **3. Load Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.1. Import Dataset**"
      ],
      "metadata": {
        "id": "6F04GkNxjmdY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import dataset\n",
        "dataset_path = '/content/drive/MyDrive/Data-TA/data-final/FIX/'\n",
        "# Memuat semua DataFrame\n",
        "df_news = pd.read_csv(dataset_path + 'ALL_NEWS2.csv', encoding='ISO-8859-1')\n",
        "df_news.drop(\"Unnamed: 0\", axis=1, inplace=True)  # Menghapus kolom \"Unnamed: 0\" secara in-place\n",
        "print(df_news)"
      ],
      "metadata": {
        "id": "lY-IwwjuisR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sqp8BiBBK6eO"
      },
      "source": [
        "### **3.2. Show Data on Chart**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Histogram"
      ],
      "metadata": {
        "id": "sUlp9m5IjsG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.value_counts(df_news[\"Label\"]).plot.bar()\n",
        "plt.title(\"Label Comparison\")\n",
        "plt.xlabel(\"Label\")\n",
        "plt.ylabel(\"Count\")\n",
        "\n",
        "# if you want to save this chart, uncomment line below\n",
        "# plt.savefig('label_histogram', dpi=300)\n",
        "\n",
        "df_news[\"Label\"].value_counts()"
      ],
      "metadata": {
        "id": "gjVbEyVTixDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5V0Di4WdLtEy"
      },
      "source": [
        "## **4. Text Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import dataset\n",
        "df_raw = dataset_path + 'ALL_NEWS2.csv'\n",
        "df_raw = pd.read_csv(df_raw, encoding='ISO-8859-1')\n",
        "df_raw.drop(\"Unnamed: 0\", axis=1, inplace=True)  # Menghapus kolom \"Unnamed: 0\" secara in-place\n",
        "df_raw.rename({\"Content\": \"Raw_Content\"}, axis=1, inplace=True)\n",
        "print(df_raw)"
      ],
      "metadata": {
        "id": "oqJHAY3zlLYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4.1. Data Cleaning**"
      ],
      "metadata": {
        "id": "JsJHUyR04mZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_cleaning(text):\n",
        "    url_regex = \"((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+)||(http\\S+))\"\n",
        "\n",
        "    text = re.sub(\"REFERENSI\", \"\", text)  # remove sentence\n",
        "    text = re.sub(\"referensi\", \"\", text)  # remove sentence\n",
        "    text = re.sub(\"Referensi\", \"\", text)  # remove sentence\n",
        "    text = re.sub(\"Copyright 2008 - 2023 PT. Kompas Cyber Media (Kompas Gramedia Digital Group). All Rights Reserved.\", \"\", text)  # remove sentence\n",
        "    text = re.sub(url_regex, \"\", text)  # remove every url\n",
        "    text = re.sub(r\"[^A-Za-z0-9\\s]\", \" \", text)  # remove character except alphabet, number, & spasi\n",
        "    text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", text)  # remove character ASCII 127\n",
        "    text = re.sub(r\"\\n\", \" \", text)  # remove every new line '\\n'\n",
        "    text = re.sub(r\"@[A-Za-z0-9]+\", \" \", text)  # remove twitter username\n",
        "    text = re.sub(\"@[\\w\\-]+\", \" \", text)  # remove mentions\n",
        "    text = re.sub(\" URL\", \" \", text)  # remove word URL\n",
        "    text = re.sub(\" url\", \" \", text)  # remove word url\n",
        "    text = re.sub(\"\\+\", \" \", text)  # remove backslash\n",
        "    text = re.sub(\"\\s+\", \" \", text)  # remove special regular expression character\n",
        "    text = re.sub(\"[^0-9a-zA-Z]\", \" \", text)  # remove punctuation\n",
        "    text = re.sub(\"[^a-zA-Z]\", \" \", text)  # remove numbers\n",
        "    text = re.sub(\" +\", \" \", text)  # remove extra spaces\n",
        "\n",
        "    return text\n",
        "\n",
        "df_news[\"Cleaning\"] = df_news[\"Content\"].apply(data_cleaning)\n",
        "df_news.Conten = (df_news.Content.str.strip())\n",
        "\n",
        "df1 = df_raw[[\"Raw_Content\"]]\n",
        "df2 = df_news[[\"Cleaning\"]]\n",
        "\n",
        "df_compare = pd.concat([df1, df2], axis=1)\n",
        "df_compare.head()"
      ],
      "metadata": {
        "id": "stSk_Z93lUII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4.2. Case Folding**"
      ],
      "metadata": {
        "id": "4IG_TZLn3OJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Case Folding\n",
        "def case_folding(text):\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "df_news[\"Casefold\"] = df_news[\"Cleaning\"].apply(case_folding)\n",
        "\n",
        "df1 = df_news[[\"Cleaning\"]]\n",
        "df2 = df_news[[\"Casefold\"]]\n",
        "\n",
        "df_compare = pd.concat([df1, df2], axis=1)\n",
        "df_compare.head()"
      ],
      "metadata": {
        "id": "dyU_XftFlZi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4.3. Tokenizing**"
      ],
      "metadata": {
        "id": "lw2xuVEj6ndv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizing\n",
        "nltk.download('punkt')  # Download the 'punkt' resource\n",
        "df_news[\"Token\"] = df_news[\"Casefold\"].apply(nltk.word_tokenize)\n",
        "\n",
        "df1 = df_news[[\"Casefold\"]]\n",
        "df2 = df_news[[\"Token\"]]\n",
        "\n",
        "df_compare = pd.concat([df1, df2], axis=1)\n",
        "\n",
        "# Set option to display the entire content of the columns\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# Display only the first row of df_compare\n",
        "df_compare_first_row = df_compare.head(1)\n",
        "print(df_compare_first_row)"
      ],
      "metadata": {
        "id": "kELVWT23lr-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqsbQ2qLYcOE"
      },
      "source": [
        "### **4.3. Normalization**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import key normalization\n",
        "key_norm = \"https://raw.githubusercontent.com/okkyibrohim/id-multi-label-hate-speech-and-abusive-language-detection/master/new_kamusalay.csv\"\n",
        "key_norm = pd.read_csv(key_norm, encoding=\"latin-1\", header=None)\n",
        "key_norm = key_norm.rename(columns={0: \"original\", 1: \"replacement\"})\n",
        "\n",
        "print(\"Number of Data: \", len(key_norm))\n",
        "key_norm.head()"
      ],
      "metadata": {
        "id": "Vnq3bE7sl4HI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "key_norm_map = dict(zip(key_norm[\"original\"], key_norm[\"replacement\"]))\n",
        "\n",
        "def normalize_token(token):\n",
        "    return key_norm_map.get(token, token)\n",
        "\n",
        "def normalize_tokens(tokens):\n",
        "    return [normalize_token(token) for token in tokens]\n",
        "\n",
        "# Apply Normalization\n",
        "df_news[\"Normalize\"] = df_news[\"Token\"].apply(normalize_tokens)\n",
        "\n",
        "df1 = df_news[[\"Token\"]]\n",
        "df2 = df_news[[\"Normalize\"]]\n",
        "\n",
        "df_compare = pd.concat([df1, df2], axis=1)\n",
        "# df_compare.head()\n",
        "\n",
        "# Set option to display the entire content of the columns\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# Display only the first row of df_compare\n",
        "df_compare_first_row = df_compare.head(1)\n",
        "print(df_compare_first_row)"
      ],
      "metadata": {
        "id": "UVm17wqsl7HG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mGRRYRhlyO7"
      },
      "source": [
        "### **4.4. Filtering**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initiate NTLK stopword\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "def remove_nltk_stopword(tokens):\n",
        "    \"\"\"Remove NLTK Stopword/Filtering.\"\"\"\n",
        "    nltk_stopword_dict = set(stopwords.words(\"indonesian\"))\n",
        "    filtered_tokens = [word for word in tokens if word not in nltk_stopword_dict]\n",
        "    filtered_tokens = [re.sub(\"  +\", \" \", token).strip() for token in filtered_tokens if token]\n",
        "    return filtered_tokens\n",
        "\n",
        "# Apply Filtering\n",
        "df_news[\"Filter\"] = df_news[\"Normalize\"].apply(remove_nltk_stopword)\n",
        "\n",
        "df1 = df_news[\"Normalize\"]\n",
        "df2 = df_news[\"Filter\"]\n",
        "\n",
        "df_compare = pd.concat([df1, df2], axis=1)\n",
        "# Set option to display the entire content of the columns\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# Display only the first row of df_compare\n",
        "df_compare_first_row = df_compare.head(1)\n",
        "print(df_compare_first_row)"
      ],
      "metadata": {
        "id": "FwaUhHaxmMia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eTW06Jhpwbw"
      },
      "source": [
        "### **4.5. Stemming**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create stemmer\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "\n",
        "def stemming(tokens):\n",
        "    \"\"\"Stemming.\"\"\"\n",
        "\n",
        "    return [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "# Apply Stemming\n",
        "df_news[\"Stem\"] = df_news[\"Filter\"].apply(stemming)\n",
        "\n",
        "df1 = df_news[[\"Filter\"]]\n",
        "df2 = df_news[[\"Stem\"]]\n",
        "\n",
        "df_compare = pd.concat([df1, df2], axis=1)\n",
        "df_compare.head()"
      ],
      "metadata": {
        "id": "sA5v7Exrmd1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate the lists of stemmed tokens back into sentences\n",
        "df_news[\"Preprocess\"] = df_news[\"Stem\"].apply(lambda tokens: ' '.join(tokens))\n",
        "print(df_news[\"Preprocess\"])"
      ],
      "metadata": {
        "id": "wzJUZwN2mj42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_news.to_csv(dataset_path + 'dataset_preprocessed_result.csv', index=False)\n",
        "print(\"Data berhasil disimpan ke dalam CSV.\")"
      ],
      "metadata": {
        "id": "AZvXmdYIml6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Za2FphIHqQiY"
      },
      "source": [
        "## **5. Finalisasi Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5.1. Preview After Preprocessing**"
      ],
      "metadata": {
        "id": "g9YDfxfkjvEX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = '/content/drive/MyDrive/Data-TA/data-final/FIX/'\n",
        "# df_news = df_news[[\"Content\", \"Label\"]]\n",
        "df_news = pd.read_csv(dataset_path + 'dataset_preprocessed_result.csv', encoding='ISO-8859-1')\n",
        "\n",
        "# Menyimpan kolom yang ingin dipertahankan\n",
        "selected_columns = [\"Sumber\", \"Title\", \"Date\", \"Preprocess\", \"Label\"]\n",
        "\n",
        "# Memilih hanya kolom yang diinginkan\n",
        "df_news = df_news[selected_columns]\n",
        "\n",
        "# Menampilkan DataFrame yang telah diubah\n",
        "df_news.head()"
      ],
      "metadata": {
        "id": "E5I6w4MGms4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5.2. Label Classification**"
      ],
      "metadata": {
        "id": "zyKydnRnYgHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def label_classification(news):\n",
        "    label = \"\"\n",
        "    if int(news) == 1:\n",
        "        label = \"Non_Hoax\"\n",
        "    else:\n",
        "        label = \"Hoax\"\n",
        "    return label\n",
        "\n",
        "# Menambahkan kolom 'Kategori' berdasarkan nilai 'Label' di kolom paling kanan\n",
        "df_news.insert(len(df_news.columns), 'Category', df_news['Label'].apply(label_classification))\n",
        "\n",
        "# Rename columns\n",
        "df_news.rename(columns={\"Sumber\": \"Source\"}, inplace=True)\n",
        "df_news.head()"
      ],
      "metadata": {
        "id": "P8CJlKV-mypS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_news.to_csv(dataset_path + 'dataset_preprocess_label.csv', index=False)"
      ],
      "metadata": {
        "id": "0DqEbOK0i97l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdWJzMpALMKL"
      },
      "source": [
        "### 5.3. Data Exploratory"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = '/content/drive/MyDrive/Data-TA/data-final/FIX/'\n",
        "df_news = pd.read_csv(dataset_path + 'dataset_preprocess_label.csv', encoding='ISO-8859-1')\n",
        "# df_news.head()"
      ],
      "metadata": {
        "id": "nwzZzARgjZxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Word Cloud\n",
        "text = ' '.join(df_news['Preprocess'].astype(str).values)\n",
        "wordcloud = WordCloud(\n",
        "    width=3000,\n",
        "    height=2000,\n",
        "    background_color='white',\n",
        "    stopwords=set(stopwords.words(\"english\"))\n",
        ").generate(text)\n",
        "\n",
        "# Display the Word Cloud\n",
        "fig = plt.figure(figsize=(10, 7.5), facecolor='k', edgecolor='k')\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.tight_layout(pad=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FWaZo51wm4MB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menampilkan informasi dasar tentang dataset\n",
        "print(\"Info Dataset:\")\n",
        "print(df_news.info())"
      ],
      "metadata": {
        "id": "3BVtghYhm_Z9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menampilkan panjang teks\n",
        "df_news['Content_length'] = df_news['Preprocess'].apply(len)\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.histplot(data=df_news, x='Content_length', bins=30)\n",
        "plt.title('Distribusi Panjang Teks')\n",
        "plt.xlabel('Panjang Teks')\n",
        "plt.ylabel('Jumlah')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sPKWpVeZnGQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menampilkan Jumlah Data Berdasarkan Kelas\n",
        "print(\"Number of Data: \", df_news.shape)\n",
        "\n",
        "# converting to df and assigning new name to the columns\n",
        "value_counts = df_news[\"Label\"].value_counts()\n",
        "\n",
        "df_value_counts = pd.DataFrame(value_counts)\n",
        "df_value_counts = df_value_counts.reset_index()\n",
        "df_value_counts.columns = [\"Label\", \"Counts of Label\"]  # change columns name\n",
        "df_value_counts"
      ],
      "metadata": {
        "id": "plockUtFnjYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menampilkan Jumlah Data Berdasarkan Sumber Berita\n",
        "source_value_counts = df_news[\"Source\"].value_counts()\n",
        "\n",
        "df_source_counts = pd.DataFrame(source_value_counts)\n",
        "df_source_counts = df_source_counts.reset_index()\n",
        "df_source_counts.columns = [\"Source\", \"Counts of Source\"]  # change columns name\n",
        "df_source_counts = df_source_counts.sort_values(by=\"Counts of Source\", ascending=False)  # sort by counts\n",
        "\n",
        "print(\"Number of Data: \", df_news.shape[0])\n",
        "print(\"\\nDistribution of Data based on 'Source':\")\n",
        "print(df_source_counts)"
      ],
      "metadata": {
        "id": "UIkQJg4nntIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5.3. Remove Duplicate**"
      ],
      "metadata": {
        "id": "32l9NmPxpKDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_before = pd.read_csv(dataset_path + 'dataset_preprocess_clear.csv')\n",
        "# print(df_combined)"
      ],
      "metadata": {
        "id": "NhDkWgDIpSFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hapus duplikat berdasarkan kolom 'Content'\n",
        "df_news = df_before.drop_duplicates(subset=['Preprocess'])\n",
        "\n",
        "# Tampilkan info setelah menghapus duplikat\n",
        "print(\"Jumlah baris sebelum menghapus duplikat:\", len(df_before))\n",
        "print(\"Jumlah baris setelah menghapus duplikat:\", len(df_news))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MIGmBrjpLm2",
        "outputId": "5c2dc254-b15d-42d1-9adf-2e03e7c3a6ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jumlah baris sebelum menghapus duplikat: 6074\n",
            "Jumlah baris setelah menghapus duplikat: 6035\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URO6cHjgsQx4"
      },
      "source": [
        "### **5.4. Remove Missing Values**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_indexed = df_news\n",
        "print(df_indexed.isnull().sum())"
      ],
      "metadata": {
        "id": "M9KdrjFYn03E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menghapus baris yang memiliki missing value pada kolom 'text' dan 'label', lalu reset index\n",
        "df_indexed = df_indexed.dropna(subset=['text', 'label']).reset_index(drop=True)\n",
        "\n",
        "# Menampilkan jumlah missing value setelah dihapus\n",
        "print(\"\\nJumlah missing value setelah dihapus:\")\n",
        "print(df_indexed.isnull().sum())"
      ],
      "metadata": {
        "id": "4EPSXswtn3k0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menyimpan DataFrame yang telah diubah ke file CSV\n",
        "df_indexed.to_csv(dataset_path + 'dataset_preprocess_clear.csv', index=False)\n",
        "print(\"Preprocessed Dataset has been downloaded\")"
      ],
      "metadata": {
        "id": "3EOJV9KVn8Ui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5da_f-Pr4-cv"
      },
      "source": [
        "## **6. Split Up Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6.1. Import Preprocessed Dataset**"
      ],
      "metadata": {
        "id": "SWK8T2EGj3Xr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import preprocessed dataset\n",
        "dataset_path = '/content/drive/MyDrive/Data-TA/data-final/FIX/'\n",
        "df = pd.read_csv(dataset_path + 'dataset_preprocess_clear.csv')\n",
        "print(\"Dataset successfully imported\")"
      ],
      "metadata": {
        "id": "ppium0y6oJwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9im-oOfw5UQR"
      },
      "source": [
        "### **6.2. Hold-out Validation**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define dataset\n",
        "X = df[\"Preprocess\"]\n",
        "y = df[\"Label\"]\n",
        "\n",
        "random_state = 42\n",
        "# split into 80:10:10 ration\n",
        "X_train, X_rem, y_train, y_rem = train_test_split(\n",
        "    X, y, train_size=0.8, stratify=y, random_state=random_state\n",
        ")\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(\n",
        "    X_rem, y_rem, test_size=0.5, stratify=y_rem, random_state=random_state\n",
        ")\n",
        "\n",
        "# describe info about train, valid, and test set\n",
        "print(\"Number of Train Dataset: \")\n",
        "print(y_train.value_counts())\n",
        "\n",
        "print(\"\\nNumber of Valid Dataset: \")\n",
        "print(y_valid.value_counts())\n",
        "\n",
        "print(\"\\nNumber of Test Dataset: \")\n",
        "print(y_test.value_counts())\n",
        "\n",
        "df_train = pd.concat([X_train, y_train], axis=1)\n",
        "df_valid = pd.concat([X_valid, y_valid], axis=1)\n",
        "df_test = pd.concat([X_test, y_test], axis=1)"
      ],
      "metadata": {
        "id": "SyoKgrqcoSyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.to_csv(dataset_path + 'dataset_training.csv', index=False)\n",
        "df_valid.to_csv(dataset_path + 'dataset_validation.csv', index=False)\n",
        "df_test.to_csv(dataset_path + 'dataset_testing.csv', index=False)"
      ],
      "metadata": {
        "id": "oiK8X3z_uIgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7. Tokenization & Embedding**"
      ],
      "metadata": {
        "id": "fH4fj-MI5Igd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **7.2. Tokenization**"
      ],
      "metadata": {
        "id": "JGoLBOdgfJVK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.read_csv(dataset_path + 'dataset_training_resampled.csv')\n",
        "df_test = pd.read_csv(dataset_path + 'dataset_testing.csv')\n",
        "df_valid = pd.read_csv(dataset_path + 'dataset_validation.csv')"
      ],
      "metadata": {
        "id": "zro6l31uRAA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization using Keras Tokenizer\n",
        "features = 16670\n",
        "tokenizer = Tokenizer(num_words=features)\n",
        "tokenizer.fit_on_texts(df_train[\"Preprocess\"])\n",
        "\n",
        "word_index = tokenizer.word_index # get all words that the tokenizer knows\n",
        "print('Found %s unique tokens.' % len(word_index))"
      ],
      "metadata": {
        "id": "nRa301Aroc8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menyimpan Tokenizer ke dalam file pickle\n",
        "with open(dataset_path + 'tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# Menyimpan word_index ke dalam file JSON\n",
        "with open(dataset_path + 'word_index.json', 'w') as file:\n",
        "    json.dump(word_index, file)"
      ],
      "metadata": {
        "id": "5H1U_QfIulET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save padded sequences\n",
        "np.save(dataset_path + 'X_train_padded.npy', X_train_padded)\n",
        "np.save(dataset_path + 'X_valid_padded.npy', X_valid_padded)\n",
        "np.save(dataset_path + 'X_test_padded.npy', X_test_padded)\n",
        "\n",
        "# Save labels\n",
        "np.save(dataset_path + 'y_train.npy', y_train.values)\n",
        "np.save(dataset_path + 'y_valid.npy', y_valid.values)\n",
        "np.save(dataset_path + 'y_test.npy', y_test.values)"
      ],
      "metadata": {
        "id": "cfBvqYV1k0ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **7.3. FastText Embedding**"
      ],
      "metadata": {
        "id": "bug0lgO2fMiW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nb4gLQYD1xi"
      },
      "outputs": [],
      "source": [
        "# Load Model\n",
        "file = gzip.open(urlopen('https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.id.300.vec.gz'))\n",
        "vocab_and_vectors = {} # put words as dict indexes and vectors as words values\n",
        "\n",
        "for line in file:\n",
        "  values = line.split()\n",
        "  word = values[0].decode('utf-8')\n",
        "  vector = np.asarray(values[1:], dtype='float32')\n",
        "  vocab_and_vectors[word] = vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgPUe43YIMu_"
      },
      "outputs": [],
      "source": [
        "# Menyambungkan data ke pre-trained word embedding\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = vocab_and_vectors.get(word)\n",
        "    # words that cannot be found will be set to 0\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of word embedding matrix for a specific word\n",
        "sample_word = list(tokenizer.word_index.keys())[1]\n",
        "print(f\"Embedding matrix for word '{sample_word}': \\n\")\n",
        "print(embedding_matrix[tokenizer.word_index[sample_word]])"
      ],
      "metadata": {
        "id": "oisjrPoRpCaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menyimpan embedding matrix\n",
        "dataset_path = '/content/drive/MyDrive/Data-TA/data-final/FIX/C4V3/'\n",
        "np.save(dataset_path + 'embedding_matrix.npy', embedding_matrix)"
      ],
      "metadata": {
        "id": "YsR5ZrcLtnh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **8. Balancing Data with Borderline-SMOTE (only training)**"
      ],
      "metadata": {
        "id": "IbPG3Vgf3xic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_state = 42\n",
        "# Apply Borderline-SMOTE to training data only\n",
        "smote = BorderlineSMOTE(random_state=random_state)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_padded, y_train)"
      ],
      "metadata": {
        "id": "XvHw9x5536YM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Pembagian Kelas Sebelum Oversampling (Training Set):\")\n",
        "print(\"Counts of '0': {}\".format(sum(y_train == 0)))\n",
        "print(\"Counts of '1': {}\".format(sum(y_train == 1)))\n",
        "print(\"Total: {}\".format(len(y_train)))\n",
        "\n",
        "print(\"Pembagian Kelas Setelah Oversampling (Training Set):\")\n",
        "print(\"Counts of '0': {}\".format(sum(y_train_resampled == 0)))\n",
        "print(\"Counts of '1': {}\".format(sum(y_train_resampled == 1)))\n",
        "print(\"Total: {}\".format(len(y_train_resampled)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVcIncNsLivg",
        "outputId": "89369573-bf26-4281-8ce2-61f10884ed3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pembagian Kelas Sebelum Oversampling (Training Set):\n",
            "Counts of '0': 162\n",
            "Counts of '1': 4666\n",
            "Total: 4828\n",
            "Pembagian Kelas Setelah Oversampling (Training Set):\n",
            "Counts of '0': 4666\n",
            "Counts of '1': 4666\n",
            "Total: 9332\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Menyimpan X_train_resampled\n",
        "np.save(dataset_path + 'X_train_resampled.npy', X_train_resampled)\n",
        "\n",
        "# Menyimpan y_train_resampled\n",
        "np.save(dataset_path + 'y_train_resampled.npy', y_train_resampled)"
      ],
      "metadata": {
        "id": "XIBU5wYWvm1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUUsdBmAyu2Z"
      },
      "source": [
        "## **9. Pemodelan**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **9.1 Load Data**"
      ],
      "metadata": {
        "id": "RSvD3fDfpXpi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Memuat word_index dari file JSON\n",
        "dataset_path = '/content/drive/MyDrive/Data-TA/data-final/FIX/C4V3/'\n",
        "with open(dataset_path + 'word_index.json', 'r') as file:\n",
        "    word_index = json.load(file)"
      ],
      "metadata": {
        "id": "M_qvVIEvvAP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tentukan path dataset\n",
        "dataset_path = '/content/drive/MyDrive/Data-TA/data-final/FIX/C4V3/'\n",
        "\n",
        "# Load padded sequences\n",
        "X_train_padded = np.load(dataset_path + 'X_train_padded.npy')\n",
        "X_valid_padded = np.load(dataset_path + 'X_valid_padded.npy')\n",
        "X_test_padded = np.load(dataset_path + 'X_test_padded.npy')\n",
        "\n",
        "# Load labels\n",
        "y_train = pd.Series(np.load(dataset_path + 'y_train.npy'))\n",
        "y_valid = pd.Series(np.load(dataset_path + 'y_valid.npy'))\n",
        "y_test = pd.Series(np.load(dataset_path + 'y_test.npy'))"
      ],
      "metadata": {
        "id": "Sbcco2C0vKZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Memuat embedding matrix\n",
        "embedding_matrix = np.load(dataset_path + 'embedding_matrix.npy')"
      ],
      "metadata": {
        "id": "RZDh0n5CvEH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Memuat X_train_resampled\n",
        "X_train_resampled = np.load(dataset_path + 'X_train_resampled.npy')\n",
        "# Memuat y_train_resampled\n",
        "y_train_resampled = np.load(dataset_path + 'y_train_resampled.npy')"
      ],
      "metadata": {
        "id": "Jb7vuOPZv0MM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **9.1 Hyperparameter Tuning**"
      ],
      "metadata": {
        "id": "W2ZpBWsTj-BW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlHPEXVq2V0t"
      },
      "source": [
        "#### *Learning Rate*"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Daftar Learning Rate yang akan diuji\n",
        "learning_rates = [0.01, 0.001, 0.0001]\n",
        "\n",
        "# Inisialisasi variabel untuk menyimpan hasil terbaik\n",
        "best_accuracy = 0\n",
        "best_loss = float('inf')\n",
        "best_learning_rate = None\n",
        "\n",
        "# Looping untuk hyperparameter tuning\n",
        "for learning_rate in learning_rates:\n",
        "    print(f\"\\nTraining model with learning rate: {learning_rate}\")\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(len(word_index) + 1, 300,\n",
        "                        input_length=X_train_padded.shape[1],\n",
        "                        weights=[embedding_matrix],\n",
        "                        trainable=False))\n",
        "    model.add(Bidirectional(LSTM(64)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer=optimizer,\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    es = EarlyStopping(restore_best_weights=True,\n",
        "                       monitor='val_loss', mode='min',\n",
        "                       verbose=1, patience=10)\n",
        "\n",
        "    print(model.summary())\n",
        "\n",
        "    history = model.fit(X_train_resampled, y_train_resampled,\n",
        "                        epochs=100,\n",
        "                        batch_size=32, verbose=1,\n",
        "                        validation_data=(X_valid_padded, y_valid),\n",
        "                        callbacks=[es])\n",
        "\n",
        "    # Evaluate model on validation data\n",
        "    val_loss, val_accuracy = model.evaluate(X_valid_padded, y_valid)\n",
        "\n",
        "    # Print validation accuracy\n",
        "    print(f'Validation Accuracy with learning rate {learning_rate}: {val_accuracy}')\n",
        "\n",
        "    # Classification report on validation data\n",
        "    y_val_pred_prob = model.predict(X_valid_padded)\n",
        "    y_val_pred = (y_val_pred_prob > 0.5).astype(int)\n",
        "    print('\\nClassification Report (Validation) :')\n",
        "    print(classification_report(y_valid, y_val_pred, digits=4))\n",
        "\n",
        "    # Plot training vs validation accuracy\n",
        "    plt.figure(figsize=(16, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend(['accuracy', 'val_accuracy'])\n",
        "    plt.title(f\"Accuracy with learning rate {learning_rate}\")\n",
        "\n",
        "    # Plot training vs validation loss\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend(['loss', 'val_loss'])\n",
        "    plt.title(f\"Loss with learning rate {learning_rate}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    # Check if the current model is better than the previous best\n",
        "    if val_loss < best_loss:\n",
        "        best_loss = val_loss\n",
        "        best_accuracy = val_accuracy\n",
        "        best_learning_rate = learning_rate\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"\\nBest Hyperparameters:\")\n",
        "print(f\"Best Learning Rate: {best_learning_rate}\")\n",
        "print(f\"Best Validation Accuracy: {best_accuracy}\")\n",
        "print(f\"Best Validation Loss: {best_loss}\")"
      ],
      "metadata": {
        "id": "ZlWk_nIApqKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnScT9aP2fIl"
      },
      "source": [
        "#### *Batch Size*"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Daftar ukuran batch yang akan diuji\n",
        "batch_sizes = [16, 32, 64]\n",
        "\n",
        "# Inisialisasi variabel untuk menyimpan hasil terbaik\n",
        "best_accuracy = 0\n",
        "best_loss = float('inf')\n",
        "best_batch_size = None\n",
        "\n",
        "# Looping untuk hyperparameter tuning\n",
        "for batch_size in batch_sizes:\n",
        "    print(f\"\\nTraining model with Batch Size: {batch_size}\")\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(len(word_index) + 1, 300,\n",
        "                        input_length=X_train_padded.shape[1],\n",
        "                        weights=[embedding_matrix],\n",
        "                        trainable=False))\n",
        "    model.add(Bidirectional(LSTM(64)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    optimizer = Adam(learning_rate=0.0001)  # Tetapkan learning rate, bisa disesuaikan\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer=optimizer,\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    es = EarlyStopping(restore_best_weights=True,\n",
        "                       monitor='val_loss', mode='min',\n",
        "                       verbose=1, patience=10)\n",
        "\n",
        "    print(model.summary())\n",
        "\n",
        "    history = model.fit(X_train_resampled, y_train_resampled,\n",
        "                        epochs=100,\n",
        "                        batch_size=batch_size, verbose=1,\n",
        "                        validation_data=(X_valid_padded, y_valid),\n",
        "                        callbacks=[es])\n",
        "\n",
        "    # Evaluate model on validation data\n",
        "    val_loss, val_accuracy = model.evaluate(X_valid_padded, y_valid)\n",
        "\n",
        "    # Print validation accuracy\n",
        "    print(f'Validation Accuracy with Batch Size {batch_size}: {val_accuracy}')\n",
        "\n",
        "    # Classification report on validation data\n",
        "    y_val_pred_prob = model.predict(X_valid_padded)\n",
        "    y_val_pred = (y_val_pred_prob > 0.5).astype(int)\n",
        "    print('\\nClassification Report (Validation) :')\n",
        "    print(classification_report(y_valid, y_val_pred, digits=4))\n",
        "\n",
        "    # Plot training vs validation accuracy\n",
        "    plt.figure(figsize=(16, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend(['accuracy', 'val_accuracy'])\n",
        "    plt.title(f\"Accuracy with Batch Size {batch_size}\")\n",
        "\n",
        "    # Plot training vs validation loss\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend(['loss', 'val_loss'])\n",
        "    plt.title(f\"Loss with Batch Size {batch_size}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    # Check if current model is better than the previous best\n",
        "    if val_loss < best_loss:\n",
        "        best_loss = val_loss\n",
        "        best_accuracy = val_accuracy\n",
        "        best_batch_size = batch_size\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"\\nBest Hyperparameters:\")\n",
        "print(f\"Best Batch Size: {best_batch_size}\")\n",
        "print(f\"Best Validation Accuracy: {best_accuracy}\")\n",
        "print(f\"Best Validation Loss: {best_loss}\")"
      ],
      "metadata": {
        "id": "j6K87QYvpufr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kW52wPmb2je4"
      },
      "source": [
        "#### *Unit BiLSTM*"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Daftar jumlah unit BiLSTM yang akan diuji\n",
        "units_list = [32, 64, 128]\n",
        "\n",
        "# Inisialisasi variabel untuk menyimpan hasil terbaik\n",
        "best_accuracy = 0\n",
        "best_loss = float('inf')\n",
        "best_units = None\n",
        "\n",
        "# Looping untuk hyperparameter tuning\n",
        "for units in units_list:\n",
        "    print(f\"\\nTraining model with BiLSTM units: {units}\")\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(len(word_index) + 1, 300,\n",
        "                        input_length=X_train_padded.shape[1],\n",
        "                        weights=[embedding_matrix],\n",
        "                        trainable=False))\n",
        "    model.add(Bidirectional(LSTM(units)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    optimizer = Adam(learning_rate=0.0001)\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer=optimizer,\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    es = EarlyStopping(restore_best_weights=True,\n",
        "                       monitor='val_loss', mode='min',\n",
        "                       verbose=1, patience=10)\n",
        "\n",
        "    print(model.summary())\n",
        "\n",
        "    history = model.fit(X_train_resampled, y_train_resampled,\n",
        "                        epochs=100,\n",
        "                        batch_size=16, verbose=1,\n",
        "                        validation_data=(X_valid_padded, y_valid),\n",
        "                        callbacks=[es])\n",
        "\n",
        "    # Evaluate model on validation data\n",
        "    val_loss, val_accuracy = model.evaluate(X_valid_padded, y_valid)\n",
        "\n",
        "    # Print validation accuracy\n",
        "    print(f'Validation Accuracy with {units} units: {val_accuracy}')\n",
        "\n",
        "    # Classification report on validation data\n",
        "    y_val_pred_prob = model.predict(X_valid_padded)\n",
        "    y_val_pred = (y_val_pred_prob > 0.5).astype(int)\n",
        "    print('\\nClassification Report (Validation) :')\n",
        "    print(classification_report(y_valid, y_val_pred, digits=4))\n",
        "\n",
        "    # Plot training vs validation accuracy\n",
        "    plt.figure(figsize=(16, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend(['accuracy', 'val_accuracy'])\n",
        "    plt.title(f\"Accuracy with {units} units\")\n",
        "\n",
        "    # Plot training vs validation loss\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend(['loss', 'val_loss'])\n",
        "    plt.title(f\"Loss with {units} units\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    # Check if current model is better than the previous best\n",
        "    if val_loss < best_loss:\n",
        "        best_loss = val_loss\n",
        "        best_accuracy = val_accuracy\n",
        "        best_units = units\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"\\nBest Hyperparameters:\")\n",
        "print(f\"Best Number of BiLSTM Units: {best_units}\")\n",
        "print(f\"Best Validation Accuracy: {best_accuracy}\")\n",
        "print(f\"Best Validation Loss: {best_loss}\")"
      ],
      "metadata": {
        "id": "azJXKV5Kp9Al"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xNRI3fA3Fug"
      },
      "source": [
        "#### *Dropout*"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Daftar jumlah dropout yang akan diuji\n",
        "dropout_list = [0.3, 0.4, 0.5]\n",
        "\n",
        "# Inisialisasi variabel untuk menyimpan hasil terbaik\n",
        "best_accuracy = 0\n",
        "best_loss = float('inf')\n",
        "best_dropout = None\n",
        "\n",
        "# Looping untuk hyperparameter tuning\n",
        "for dropout_rate in dropout_list:\n",
        "    print(f\"\\nTraining model with Dropout rate: {dropout_rate}\")\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(len(word_index) + 1, 300,\n",
        "                        input_length=X_train_padded.shape[1],\n",
        "                        weights=[embedding_matrix],\n",
        "                        trainable=False))\n",
        "    model.add(Bidirectional(LSTM(128)))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    optimizer = Adam(learning_rate=0.0001)  # Tetapkan learning rate, bisa disesuaikan\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer=optimizer,\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    es = EarlyStopping(restore_best_weights=True,\n",
        "                       monitor='val_loss', mode='min',\n",
        "                       verbose=1, patience=10)\n",
        "\n",
        "    print(model.summary())\n",
        "\n",
        "    history = model.fit(X_train_resampled, y_train_resampled,\n",
        "                        epochs=100,\n",
        "                        batch_size=16, verbose=1,\n",
        "                        validation_data=(X_valid_padded, y_valid),\n",
        "                        callbacks=[es])\n",
        "\n",
        "    # Evaluate model on validation data\n",
        "    val_loss, val_accuracy = model.evaluate(X_valid_padded, y_valid)\n",
        "\n",
        "    # Print validation accuracy\n",
        "    print(f'Validation Accuracy with Dropout rate {dropout_rate}: {val_accuracy}')\n",
        "\n",
        "    # Classification report on validation data\n",
        "    y_val_pred_prob = model.predict(X_valid_padded)\n",
        "    y_val_pred = (y_val_pred_prob > 0.5).astype(int)\n",
        "    print('\\nClassification Report (Validation):')\n",
        "    print(classification_report(y_valid, y_val_pred, digits=4))\n",
        "\n",
        "    # Plot training vs validation accuracy\n",
        "    plt.figure(figsize=(16, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend(['accuracy', 'val_accuracy'])\n",
        "    plt.title(f\"Accuracy with Dropout rate {dropout_rate}\")\n",
        "\n",
        "    # Plot training vs validation loss\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend(['loss', 'val_loss'])\n",
        "    plt.title(f\"Loss with Dropout rate {dropout_rate}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    # Check if current model is better than the previous best\n",
        "    if val_loss < best_loss:\n",
        "        best_loss = val_loss\n",
        "        best_accuracy = val_accuracy\n",
        "        best_dropout = dropout_rate\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"\\nBest Hyperparameters:\")\n",
        "print(f\"Best Dropout Rate: {best_dropout}\")\n",
        "print(f\"Best Validation Accuracy: {best_accuracy}\")\n",
        "print(f\"Best Validation Loss: {best_loss}\")"
      ],
      "metadata": {
        "id": "731slwPqqFSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIJvyqF-3Y9e"
      },
      "source": [
        "### **9.2. Training Model**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Definition\n",
        "model = Sequential()\n",
        "model.add(Embedding(len(word_index) + 1, 300,\n",
        "                    input_length=X_train_padded.shape[1], weights=[embedding_matrix],\n",
        "                    trainable=False))\n",
        "model.add(Bidirectional(LSTM(128)))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile Model\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Define Callbacks\n",
        "es = EarlyStopping(restore_best_weights=True,\n",
        "                   monitor='val_loss', mode='min',\n",
        "                   verbose=1, patience=10)\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "# Fitting Model\n",
        "history = model.fit(X_train_resampled, y_train_resampled,\n",
        "                    epochs=100, batch_size=16, verbose=1,\n",
        "                    validation_data=(X_valid_padded, y_valid),\n",
        "                    callbacks=[es])"
      ],
      "metadata": {
        "id": "jUIW-CdKqIXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNPJVcx_36mL"
      },
      "source": [
        "### **9.3. Evaluasi Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### *on Validation Data*"
      ],
      "metadata": {
        "id": "vaz0X2f4nqqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Model (Validation Set)\n",
        "score = model.evaluate(X_valid_padded, y_valid, verbose=1)\n",
        "print('Loss and Accuracy Validation: ')\n",
        "print(str(model.metrics_names[0]) + \": \" + str(score[0]) + \" | \" +\n",
        "      str(model.metrics_names[1]) + \": \" + str(score[1] * 100))"
      ],
      "metadata": {
        "id": "A67LiUFoqPM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification Report\n",
        "y_pred = model.predict(X_valid_padded)\n",
        "y_pred = (y_pred > 0.5).astype(int)\n",
        "# cm = confusion_matrix(y_valid, y_pred)\n",
        "# print(cm)\n",
        "\n",
        "print(classification_report(y_valid, y_pred,\n",
        "                            digits=4, output_dict=False))\n",
        "report = classification_report(y_valid, y_pred,\n",
        "                               digits=4, output_dict=False)"
      ],
      "metadata": {
        "id": "ewM2A56mqQmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training vs validation accuracy\n",
        "plt.figure(figsize=(16, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend(['accuracy', 'val_accuracy'])\n",
        "plt.title(f\"Plot Training vs Validation Accuracy\")\n",
        "\n",
        "# Plot training vs validation loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend(['loss', 'val_loss'])\n",
        "plt.title(f\"Plot Training vs Validation Loss\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Tuw1jHtpqSrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### *on Testing Data*"
      ],
      "metadata": {
        "id": "jZw3dRd-ohnk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Model (Test Set)\n",
        "score_test = model.evaluate(X_test_padded, y_test, verbose=1)\n",
        "print('Loss and Accuracy Testing: ')\n",
        "print(str(model.metrics_names[0]) + \": \" + str(score_test[0]) + \" | \" +\n",
        "      str(model.metrics_names[1]) + \": \" + str(score_test[1] * 100))"
      ],
      "metadata": {
        "id": "ICVGaigsqUW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification Report\n",
        "y_test_pred_prob = model.predict(X_test_padded)\n",
        "y_test_pred = (y_test_pred_prob > 0.5).astype(int)\n",
        "\n",
        "report_test = classification_report(y_test, y_test_pred,\n",
        "                                    digits=4, output_dict=False)\n",
        "# Showing Classification Report\n",
        "print('Classification Report - Test Set:')\n",
        "print(report_test)"
      ],
      "metadata": {
        "id": "dBsgr0NDqVsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification Report on Testing Data\n",
        "cm_test = confusion_matrix(y_test, y_test_pred)\n",
        "print(cm_test)\n",
        "\n",
        "# Plot Confusion Matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_test, annot=True, fmt='d',\n",
        "            cmap='Blues', xticklabels=['Berita Hoax', 'Berita Non-Hoax'],\n",
        "            yticklabels=['Berita Hoax', 'Berita Non-Hoax'])\n",
        "plt.title('Confusion Matrix - Test Set')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qohqUbYbqXFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **9.4. Save Model**"
      ],
      "metadata": {
        "id": "wCsZnjAMWIrK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(dataset_path + 'hoax_classifier_FIXEDsmotetrain_p10_model.h5')\n",
        "tf.keras.models.save_model(model, dataset_path + 'hoax_classifier_FIXEDsmotetrain_p10_model.hdf5')"
      ],
      "metadata": {
        "id": "mh0AGhyIqdQD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}